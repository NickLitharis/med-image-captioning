{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from tensorflow.keras.layers import Input,Embedding,LSTM,Dense,add,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to the text file containing image names\n",
    "file_path = r'D:\\med-image-captioning\\data\\captions\\random_sample.txt'\n",
    "\n",
    "# Read the file and extract image names\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "image_names = [line.split(',')[0] for line in lines]\n",
    "\n",
    "# Define the source and destination folder paths\n",
    "source_folder_path = r'D:\\thesis\\medicat_release\\release\\figures'\n",
    "destination_folder_path = r'D:\\med-image-captioning\\images'\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(destination_folder_path, exist_ok=True)\n",
    "\n",
    "# Iterate over the list of image names\n",
    "for image_name in image_names:\n",
    "    source_image_path = os.path.join(source_folder_path, image_name)\n",
    "    \n",
    "    # Check if the image exists in the source folder\n",
    "    if os.path.exists(source_image_path):\n",
    "        destination_image_path = os.path.join(destination_folder_path, image_name)\n",
    "        \n",
    "        # Copy the image to the destination folder\n",
    "        shutil.copy2(source_image_path, destination_image_path)\n",
    "        print(f\"Copied {image_name} to destination folder.\")\n",
    "    else:\n",
    "        print(f\"Image {image_name} not found in source folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "# restarcture the model\n",
    "model = Model(inputs=model.inputs,outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract features from each photo in the directory\n",
    "features={}\n",
    "directory=r'D:\\med-image-captioning\\images'\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    filename=directory+'\\\\'+img_name\n",
    "\n",
    "    # load an image from file\n",
    "    image=load_img(filename,target_size=(224,224))\n",
    "\n",
    "    # convert the image pixels to a numpy array\n",
    "    image=img_to_array(image)\n",
    "\n",
    "    # reshape data for the model\n",
    "    image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "\n",
    "    # prepare the image for the VGG model\n",
    "    image=preprocess_input(image)\n",
    "\n",
    "    # get features\n",
    "    feature=model.predict(image,verbose=0)\n",
    "\n",
    "    # get image id\n",
    "    image_id=img_name.split('.')[0]\n",
    "\n",
    "    #save the features\n",
    "    features[image_id]=feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to file\n",
    "# pickle.dump(features,open(r'D:\\med-image-captioning\\data\\image_features\\features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the caption features using pickle\n",
    "with open(r'D:\\med-image-captioning\\data\\captions\\random_sample.txt','r') as file:\n",
    "    captions_doc=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of image captions\n",
    "mapping={}\n",
    "\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma\n",
    "    tokens=line.split(',')\n",
    "\n",
    "    if len(tokens) >= 2:\n",
    "        image_id,caption=tokens[0],tokens[1]\n",
    "\n",
    "        #remove filename from image_id\n",
    "        image_id=image_id.split('.')[0]\n",
    "\n",
    "\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id]=[]\n",
    "\n",
    "        mapping[image_id].append(caption)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add start and end sequence to the captions\n",
    "for key,captions in mapping.items():\n",
    "    for i in range(len(captions)):\n",
    "        captions[i]='<start> '+captions[i]+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping['0a60e80445d6be21f3da582eeeec7bec83d82e74_4-Figure1-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "for key in mapping.keys():\n",
    "    [all_captions.append(caption) for caption in mapping[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_captions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=max(len(caption.split()) for caption in all_captions)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids=list(mapping.keys())\n",
    "split=int(len(image_ids)*0.8)\n",
    "train=image_ids[:split]\n",
    "test=image_ids[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator\n",
    "def data_generator(data_keys,mapping,features,tokenizer,max_length,vocab_size,batch_size):\n",
    "    X1,X2,y= list(),list(),list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n+=1\n",
    "            captions=mapping[key]\n",
    "\n",
    "            for caption in captions:\n",
    "                sequence=tokenizer.texts_to_sequences([caption])[0]\n",
    "                \n",
    "                for i in range(1,len(sequence)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq,out_seq=sequence[:i],sequence[i]\n",
    "\n",
    "                    # pad input sequence\n",
    "                    in_seq=pad_sequences([in_seq],maxlen=max_length)[0]\n",
    "\n",
    "                    # encode output sequence\n",
    "                    out_seq=to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "\n",
    "                    # store the input and output sequence\n",
    "\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n==batch_size:\n",
    "                X1,X2,y=np.array(X1),np.array(X2),np.array(y)\n",
    "                yield [X1,X2],y\n",
    "                X1,X2,y=list(),list(),list()\n",
    "                n=0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "# image feature model\n",
    "imputs1=Input(shape=(4096,))\n",
    "fe1=Dropout(0.4)(imputs1)\n",
    "fe2=Dense(256,activation='relu')(fe1)\n",
    "\n",
    "# sequence feature layer\n",
    "inputs2=Input(shape=(max_length,))\n",
    "se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2=Dropout(0.4)(se1)\n",
    "se3=LSTM(256)(se2)\n",
    "\n",
    "# decoder\n",
    "decoder1=add([fe2,se3])\n",
    "decoder2=Dense(256,activation='relu')(decoder1)\n",
    "outputs=Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "# model definiton\n",
    "model=Model(inputs=[imputs1,inputs2],outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "# plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 5\n",
    "steps = len(train)//batch_size\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    generattor=data_generator(train,mapping,features,tokenizer,max_length,vocab_size,batch_size)\n",
    "    \n",
    "    model.fit(next(generattor), epochs=1, steps_per_epoch=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(r'D:\\med-image-captioning\\model\\model_50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_word(integer,tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def predict_caption(model,image,tokenizer,max_length):\n",
    "    in_text=\"<start>\"\n",
    "\n",
    "    for i in range(max_length):\n",
    "        sequence=tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence=pad_sequences([sequence],max_length)\n",
    "\n",
    "        yhat=model.predict([image,sequence],verbose=0)\n",
    "\n",
    "        # convert probability to integer\n",
    "        yhat=np.argmax(yhat)\n",
    "\n",
    "        # convert index to word\n",
    "        word=index_to_word(yhat,tokenizer)\n",
    "\n",
    "        if word is None:\n",
    "            break\n",
    "        ## append as input for generating the next word\n",
    "        in_text+=\" \"+word\n",
    "\n",
    "        # stop if we cannot generate the next word\n",
    "        if word=='<end>':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "actual,predicted=list(),list()\n",
    "\n",
    "for key in test:\n",
    "    try:\n",
    "        # get actual caption\n",
    "        captions=mapping[key]\n",
    "\n",
    "        # generate caption\n",
    "        y_pred=predict_caption(model,features[key],tokenizer,max_length)\n",
    "\n",
    "        #split into words\n",
    "        actual_captions=[caption.split() for caption in captions]\n",
    "        y_pred=y_pred.split()\n",
    "        #append to the list\n",
    "        actual.append(actual_captions)\n",
    "        predicted.append(y_pred)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# calculate BLEU score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# calculate BLEU score\n",
    "print(\"BLUE-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLUE-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_name):\n",
    "\n",
    "    # image_name=\"780d51420ad492a61d00295adb6919bd492e6bd3_8-Figure3-1.png\"\n",
    "    img_path=r'D:\\med-image-captioning\\images\\\\'+image_name\n",
    "    img_name=image_name.split('.')[0]\n",
    "    image=Image.open(img_path)\n",
    "    captions=mapping[img_name]\n",
    "\n",
    "    print(\"----------------Actual Captions----------------\")\n",
    "    print(captions)\n",
    "    y_pred=predict_caption(model,features[img_name],tokenizer,max_length)\n",
    "    print(\"----------------Predicted Caption----------------\")\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption('0a60e80445d6be21f3da582eeeec7bec83d82e74_4-Figure1-1.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
